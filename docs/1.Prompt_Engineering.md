## Potential Problems with Prompt Engineering

### Prompt Engineering is Brittle

- The behavior of an LLM is exquisitely sensitive to the specific wording, formatting, and examples in the prompt. A change that seems minor to a human can lead to dramatically different and often worse outputs. There's no guarantee of stability.

###  Lack of Systematic Optimization

-  Improving prompts is a manual, qualitative, and intuitive process. It relies on the developer's cleverness and anecdotal testing rather than data-driven, automated, and measurable optimization.
    - **The "Goldilocks" Problem (Manual Guesswork):**
        - You try multiple prompts and you have no statistical confidence about which prompt is better across yur entire dataset.  You've selected a "local maximum" based on a tiny, non-representative sample.
    - **The Few-Shot Example Selection Nightmare:**
        - You scroll through your dataset, pick 3 examples that seem "representative," and paste them in. Below are some problems
            - **Bias:** Your selection is inherently biased by what you've recently seen or what you find interesting.
            - **Combinatorial Explosion:** What if the order of the examples matters? What if a different set of 3 examples is 10% better? Manually testing these combinations is impossible.
            - **Context Window Waste:** You might be including redundant or unhelpful examples, wasting precious tokens and reducing performance on longer inputs.
    - **No Defined Metric or Pipeline:**
        - In traditional ML, you have a `model.fit(X_train, y_train)` function that automatically tries to minimize a loss function on the training data.
        - In manual prompt engineering, the "training loop" is: `(1) Human has an idea -> (2) Manually edits a string -> (3) Runs a few spot checks -> (4) Goto (1)`.
        - There is no automated way to score a prompt against 1000 validation examples and say, "This version has 85% accuracy, the previous one had 82%."
- **The Core Issue:** The optimization process is not software-driven. It's human-in-the-loop for the most tedious and error-prone parts, making it impossible to scale and reproduce.

### Model Upgrades are Risky

- Changing the underlying LLM (e.g., from GPT-4 to Claude 3, or from one version of Llama to the next) invalidates all the manual effort put into prompt crafting. The prompts are often overfitted to the quirks and "personality" of a specific model.
- **The Core Issue:** The logic of your application (the "what to do") is hopelessly entangled with the instructions for a specific model (the "how to say it"). Changing the model forces you to re-solve the "how to say it" problem from scratch.

## Solutions using DsPy

- Solution to [Prompt Engineering is Brittle](#prompt-engineering-is-brittle) problem is mentioned in [Brittle Solutions](2.Brittle_Solution.md)