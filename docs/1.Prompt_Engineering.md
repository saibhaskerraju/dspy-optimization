## Potential Problems with Prompt Engineering

### Model Upgrades are Risky

- Changing the underlying LLM (e.g., from GPT-4 to Claude 3, or from one version of Llama to the next) invalidates all the manual effort put into prompt crafting. The prompts are often overfitted to the quirks and "personality" of a specific model.
- **The Core Issue:** The logic of your application (the "what to do") is hopelessly entangled with the instructions for a specific model (the "how to say it"). Changing the model forces you to re-solve the "how to say it" problem from scratch.

### Prompt Engineering is Brittle

- The behavior of an LLM is exquisitely sensitive to the specific wording, formatting, and examples in the prompt. A change that seems minor to a human can lead to dramatically different and often worse outputs. There's no guarantee of stability.

###  Lack of Systematic Optimization

-  Improving prompts is a manual, qualitative, and intuitive process. It relies on the developer's cleverness and anecdotal testing rather than data-driven, automated, and measurable optimization.
    - **The "Goldilocks" Problem (Manual Guesswork):**
        - You try multiple prompts and you have no statistical confidence about which prompt is better across yur entire dataset.  You've selected a "local maximum" based on a tiny, non-representative sample.
    - **The Few-Shot Example Selection Nightmare:**
        - You scroll through your dataset, pick 3 examples that seem "representative," and paste them in. Below are some problems
            - **Bias:** Your selection is inherently biased by what you've recently seen or what you find interesting.
            - **Combinatorial Explosion:** What if the order of the examples matters? What if a different set of 3 examples is 10% better? Manually testing these combinations is impossible.
            - **Context Window Waste:** You might be including redundant or unhelpful examples, wasting precious tokens and reducing performance on longer inputs.
    - **No Defined Metric or Pipeline:**
        - In traditional ML, you have a `model.fit(X_train, y_train)` function that automatically tries to minimize a loss function on the training data.
        - In manual prompt engineering, the "training loop" is: `(1) Human has an idea -> (2) Manually edits a string -> (3) Runs a few spot checks -> (4) Goto (1)`.
        - There is no automated way to score a prompt against 1000 validation examples and say, "This version has 85% accuracy, the previous one had 82%."
- **The Core Issue:** The optimization process is not software-driven. It's human-in-the-loop for the most tedious and error-prone parts, making it impossible to scale and reproduce.

## Solutions using DsPy

- Solution to [Model Upgrade](#model-upgrades-are-risky)
    - Each LLM Provider has their own `Prompting Guide` that developers need to be aware of to get the maximum output from the LLM. [reference](https://cookbook.openai.com/examples/gpt4-1_prompting_guide)
    - Each model is trained on a dataset which is either in markdown or XML or text etc. Developers need to be aware of this and make sure the context is also passed in a certain way.
    - DsPy abstracts away all these as it takes care of them for us thus taking care of changing models at anytime and worrying only about `what we need` rather than `how to tell` LLM.
- Solution to [Prompt Engineering is Brittle](#prompt-engineering-is-brittle) problem is mentioned in [Brittle Solutions](2.Brittle_Solution.md)
- Solution to [Lack of Systematic Optimization](#lack-of-systematic-optimization) problem is mentioned in [Optimization Solutions](3.Optimization_Solution.md)
- Solution to [No Defined Metric](#lack-of-systematic-optimization) problem is mentioned in [Evaluation Solutions](4.Evaluation_Solution.md)